# Jacob Bernoulli
Jacob Bernoulli (1655-1705) was a Swiss Mathematician

- Discovered $e$ by looking at compound interest
- Was interested in how much one would expect to win in games of chance (only two outcomes, win or lose)

# Bernoulli Random Variable
Suppose an experiment results in Success or Failure

- $X$ is a random indicator variable, $X=1$ on success, $X=0$ on failure
- $P(X=1)=p$
- $P(X=0)=1-p$
- $X$ is a **Bernoulli** random variable
- Sometimes write $X \sim Ber(p)$

Examples:

- Coin flip
- Random binary digit
- Packet erasure in a wireless network
- Transmission by WiFi station

# Binomial Random Variable
Consider $n$ independent trials of a $Ber(p)$ random variable

- $X$ is the number of successes in $n$ trials
- $X$ is the sum of $n$ Bernoulli random variables, $X = X_{1} + X_{2} + \dots + X_{n}$, where random variable $X_{i} \sim Ber(p)$ is 1 if success in trial $i$ and 0 otherwise
- $X$ is a **Binomial** random variable: $X \sim Bin(n, p)$

$$P(X=i) = \binom{n}{i} p^{i} (1-p)^{n-i}, i=0, 1, \dots, n$$

(recall $\binom{n}{i}$ is the number of outcomes with exactly $i$ successes and $n-i$ failures)

Examples:

- number of heads in $n$ coin flips
- number of 1's in randomly generated bit string of length $n$
- number of packets erased out of a file of $n$ packets

Binomial variable $X \sim Bin(n, p)$ is sum of $n$ Bernoulli random variables $X_{i} \sim Ber(p), i=1, 2, \dots, n$

- Suppose $X \sim Bin(n_{1}, p)$ and $Y \sim Bin(n_{2}, p)$ (its important that $p$ is the same for both)
- Then $Z=X+Y \sim Bin(n_{1} + n_{2}, p)$
- $Z = X_{1}+X_{2}+\dots+X_{n1}+Y_{1}+Y_{2}+\dots+Y_{n2}$
- All terms are independent, all are $Ber(p)$

## Example: Error Correcting Codes
- Have information 4 bits to send across network
- Add 3 "parity" bits, send 7 bits total
- Each bit is independently corrupted (flipped) in transition with probability 0.1
- $X=$ number of bits corrupted, $X \sim Bin(7, 0.1)$
- Parity bits allows us to correct at most 1 error
- Probability that a correctable message is received is $P(X<2)$
- $P(X<2) = P(X=0)+P(X=1)$
    - $P(X=0)=\binom{7}{0} 0.1^{0}0.9^{7} \approx 0.48$
	- $P(X=1)=\binom{7}{1} 0.1^{1}0.9^{6} \approx 0.37$
    - $P(X=0)+P(X=1) \approx 0.85$
- Not using error correcting code, $X \sim Bin(4, 0.1)$
    - $P(X=0)=\binom{4}{0} 0.1^{0}0.9^{4} \approx 0.66$
- So error correcting code improves reliability by about 30%

# Stochastic Simulation
On a computer how can we obtain realisations of a Bernoulli random variable?

- Assume we have a random number generator that picks an integer between 0 and maxint uniformly a random. We then divide by maxint to give value $U$ between 0 and 1
- Always worth checking its not too bad though, e.g. distribution of 1M samples generated by matlab `rand()` function

## Digression: Pseudo-random number generators
- Traditional approach is to use something like: $X_{n+1} = (aX_{n} + c) \text{ mod } m$, with $a=1103515245, c=12345, m=2^{31}$
- Much better (and ore complicated) is the Mersenne Twister - used by Python, Matlab, etc.
- Crypto uses specialised PRNGs

## Bernoulli Random Variable
- Generator values with a $Ber(p)$ distribution: $X = \begin{cases} 1 & U \leq p \\ 0 & U > p\end{cases}$
- With $p=0.1$:

![$r=\text{rand}(1, 1000000); x=(r<=0.1); \text{hist}(x, [0:0.1:1])$](Stats/Diagrams/8.1.png)

## Binomial Random Variable
- Generate values with a $Bin(n, p)$ distribution
- First generate $n$ values from a $Ber(p)$ distribution, then sum them
- with $n=10$ and $p=0.3$:

![$y=[]; p=0.3; n=10; \text{ for } i=1:100000, r=\text{rand}(1, n); x=(r<=p); y=[y; \text{sum}(x)]; \text{end}; nn=\text{hist}(y, [0:10]); \text{bar}([0:10], nn)$](Stats/Diagrams/8.2.png)

- These plots are frequencies - how can we convert them to probabilities?
- Normalise so that they sum to 1
- We can then plot the distribution of our numerical samples against the true distribution as a check
